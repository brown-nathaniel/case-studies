---
title: "case1final"
author: "Nathaniel Brown, Annie Tang, William Yang"
date: "September 13, 2017"
output:
  html_document: default
  pdf_document: default
---


```{r setup, include=FALSE, echo=FALSE}
set.seed(440)
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2); library(dplyr); library(reshape2); library(knitr); library(rstan); library(lme4); library(mvtnorm)
```


```{r, warning=FALSE, echo=FALSE}
dat <- read.table("alllabs.txt", header = TRUE, stringsAsFactors = FALSE, na.strings = ".")
dat <- dat[!(is.na(dat$blot) | is.na(dat$body)),]

ndat <- nrow(dat)
trainrows <- sample(x=1:ndat, size = ceiling(0.75*ndat))
testrows <- setdiff(1:ndat, trainrows)
dat_train <- dat[trainrows,]
dat_test <- dat[testrows,]
```

### Introduction

In this report, we analyze data from an international validation study to measure the effectiveness of the rat uterotrophic bioassay. The bioassay being studied attempts to measure the estrogenic effect of certain chemicals. The two chemicals used in this study have well known effects, so we would like to verify that the bioassay produces consistent results in rats that have been administered various dosages of the chemicals across two possible protocols. If the uterotrophic bioassay is an effective procedure for measuring the effects of these chemicals, then we expect to see consistent responses to various dosages across all labs and groupings.

To measure the consistency of the responses, we fit iterations of a linear mixed effects model on the provided dataset. The model is conditioned on the labs to account for lab-to-lab variability. Each iteration of the model differs slightly in transformations and conditioning of the predictors/responses until we arrive at a model that we deem most appropriate for the dataset. We then evaluate the final model to determine whether it predicts large variation in responses to the dosages between labs, or if it predicts a consistent measured response to the chemicals.

### Model-Fitting

$$
y_{ij} \sim \beta_{0,i} + \beta_{1,i}x_{ij,d_1} + \beta_{2,i}x_{ij,d_2} + \beta_{3,i}x_{ij,p_B} + \beta_{4,i}x_{ij,p_C} + \beta_{5,i}x_{ij,p_D} + \beta_{6,i}x_{ij,log(w)} + \epsilon
\\
\beta_{0:6,i} \sim N(\mu_{0:6,i}, \sigma^2_{0:6,i})
\\
\epsilon \sim N(0, \sigma^2)
$$

Let $y_{ij}$ be the observation for log(blotted uterus weight) for subject $x_{ij}$, the $j$th individual in lab $i$. $x_{ij,d_1}$ and $x_{ij,d_2}$ are the values of dose1 and dose2 for subject $x_{ij}$. $x_{ij,p_B}$, $x_{ij,p_C}$, and $x_{ij,p_D}$ are dummy variables for which protocol $x_{ij}$ was subjected to. $x_{ij,log(w)}$ is the log(body weight) for $x_{ij}$. We make the Gaussian assumption that the coefficients, $\beta$, are normally distributed according to some $\mu_i$ and $\sigma_i$. We add a random effect on all $\beta_{0:5,i}$ to account for lab-to-lab variability in the intercepts and slopes on the blotted weight for the different dosages and protocols.

We start at a reduced form of this model and augment it to its full form after initial analyses.

```{r, echo=FALSE}
plot_lmer <- function(mod, parmfrow = NA){
  
  par(mfrow = parmfrow)

  fitvals <- exp(predict(mod, dat_train))
  resids <- summary(mod)[["residuals"]]

  plot(fitvals, resids, main = "Residuals vs. Fitted Values", xlab = "Fitted Values", ylab = "Residuals"); abline(h=0)
  qqnorm(resids, main = "Normal Quantile-Quantile Plot")
  hist(resids, main = "Histogram of Residuals", xlab="Residuals", col = "gray")
  
  continuouseffects <- grep("(dose)|(body)", colnames(coef(mod)[[1]]), value=TRUE)

  #for cases when we interact with proto
  continuouseffects <- gsub(":", "')*", continuouseffects)
  continuouseffects <- lapply(
    strsplit(continuouseffects, "proto"),
    function(l){
      if(length(l) > 1){
        return(paste0("(proto=='", l[2]))
      }
      return(l)
    }
  )
  X <- apply(as.matrix(continuouseffects), 1,
        function(x){
          with(dat_train, eval(parse(text = x)))
        }
  )
  for(c in 1:ncol(X)){
    if(grepl("proto", continuouseffects[c])){
      plot(X[,c][X[,c] > 0], resids[X[,c] > 0], main = "Residuals vs. Predictor" , xlab = continuouseffects[c])
    }else{
      plot(X[,c], resids, main = "Residuals vs. Predictor" , xlab = continuouseffects[c])
    }
  }
  par(mfrow=c(1,1))
}

```


```{r, echo=FALSE}
# RANDOM INTERCEPT MODEL
m1.reduced <- lmer(log(blot) ~ dose1 + dose2 + (1|lab), data=dat_train, REML = FALSE)
m1.full <- lmer(log(blot) ~ dose1 + dose2 + proto + (1|lab), data=dat_train, REML = FALSE)
nova <- anova(m1.reduced, m1.full)
```

In our analysis, we choose to add a random effect for the lab variable, in order to account for lab-to-lab heterogeneity. Essentially, this allows us to avoid violating an independence assumption by assuming a different baseline response for each lab. First, we model these differences between individual labs by assuming random intercepts for each lab. We first include only dose1 and dose2 as fixed effects, then introduce proto as a fixed effect after confirming through anova ($p$ < 2.2e-16, $\chi^2$=`r round(nova[["Chisq"]][length(nova[["Chisq"]])], 4)`) that it is a significant predictor. The summary statistics of this model are shows in the table below:

```{r, echo = FALSE}
lmer_list <- function(mod){
  
  fixeffmeans <- summary(mod)[["coefficients"]][,1]
  fixeffsd <- summary(mod)[["coefficients"]][,2]
  randeffmeans_all <- getME(mod, "theta")
  numdots <- sapply(names(randeffmeans_all), gregexpr, pattern="." , fixed=TRUE) %>% sapply(length)
  randeffmeans <- randeffmeans_all[which((numdots) == 1)]
  randeffvar <- diag(summary(mod)[["varcor"]][["lab"]])
  residsd <- sigma(mod)
  
  names(randeffmeans) <- names(randeffvar)

  fixefftab <- data.frame(fixeffmeans,fixeffsd^2)
  names(fixefftab) <- c("mean", "variance")
  randefftab <- data.frame(randeffmeans,randeffvar)
  names(randefftab) <- c("mean", "variance")
  residtab <- data.frame(residsd^2)
  names(residtab) <- c("variance")
  rownames(residtab) <- c("residual")
  
  return(list(fixed = fixefftab, random = randefftab, residual = residtab))  
}
format_lmer_list <- function(L = NA){
  colnames(L$fixed) <- c("Mean Fixed Effects", "Variance Fixed Effects")
  colnames(L$random) <-  c("Mean Random Effects", "Variance Random Effects")
  colnames(L$residual) <- c("Residual Variance")
  
  L <- lapply(L, round, 4)
  return(L)
}
L <- format_lmer_list(lmer_list(m1.full))
kable(L[[1]])
kable(L[[2]])
kable(L[[3]])
```


With this full model, we see that the variability due to lab is 0.028 (in terms of variance), and variability due to non-lab sources is 0.193. And when we take a look at the fixed effects, we see that an increase in the amount of dose1 corresponds to an increase (0.142 units) in the response variable, log(blot). Furthermore, an increase in the amount of dose2 corresponds to a decrease (-0.519 units) in the response variable. Protocols B, C, and D all lead to an increase in the response variable, relative to protocol A. The diagnostic plots below show that this model fits better than the previous two. Although the residual variance decreases as the fitted values increase, the quantiles of the residuals are approximately normal.

```{r fig.width=15, fig.height=10, echo=FALSE}

plot_lmer(m1.full, parmfrow=c(2,3))
```


In this random intercept model, we account for baseline differences between labs, but we also assume that the effects of doses is the same for each lab. After plotting the log(blot) versus dose by lab, we see that this is not a valid assumption to make since the lines are clearly not parallel. So we introduce a random slope model. Now, dose1 and dose2 can have varying slopes.


```{r, echo=FALSE}

ggplot(data=dat,aes(x=dose1,y=log(blot),color=lab)) + geom_point() + geom_line() + labs(x="dose 1", y="log(blot)", title="Dose 1 vs. log(blot)") + theme(plot.title = element_text(hjust = 0.5))
ggplot(data=dat,aes(x=dose2,y=log(blot),color=lab)) + geom_point() + geom_line() + labs(x="dose 2", y="log(blot)", title="Dose 2 vs. log(blot)") + theme(plot.title = element_text(hjust = 0.5))

# RANDOM SLOPE MODEL
m2 <- lmer(log(blot) ~ proto + (dose1 + dose2 + 1|lab), data=dat_train, REML=FALSE)
m3 <- lmer(log(blot) ~ proto + (I(1/(dose1+0.5)) + dose2 + 1|lab), data=dat_train, REML=FALSE)
#m3 is a much better fit FYI. normal qq is actually a straight line!
m4 <- lmer(log(blot) ~ proto + log(body) + (I(1/(dose1+1/2)) + dose2 + 1|lab), data=dat_train, REML=FALSE)
# anova to show addition of log(body) is significant 
nova <- anova(m3, m4)
```

After adding the random slopes, we transformed dose 1 by the reciprocal of (dose1 + 1/2) (we add a small number to dose because we cannot take the reciprocal of zero) and added body weight as a predictor in order to have the best fitting model. We evaluate this model using summary statistics, diagnostic plots, and out-of-sample predictive accuracy, which we evaluate using Mean Absolute Error $(\text{MAE} = E[|y - \hat{y}|])$ and Root Mean Squared Error $(\text{RMSE} = \sqrt{E[(y - \hat{y})^2]})$. Root Mean Squared Error penalizes more for extreme errors, while Mean Absolute Error simply averages all of the errors.


```{r, fig.height=10, fig.width=15, echo=FALSE}
L2 <- format_lmer_list(lmer_list(m4))
kable(L2[[1]]); kable(L2[[2]]); kable(L2[[3]])

plot_lmer(m4, parmfrow = c(2,3))

#root mean squared error penalizes more for extreme error
rms_err <- function(actual, predicted){
  error <- actual - predicted
  return(sqrt(mean((error)^2)))
}
 
#absolute error weighs all error equally
abs_err <- function(actual, predicted){
  error <- actual - predicted
  mean(abs(error))
}
 
compute_errs <- function(mod, test = dat_test){
  preds <- exp(predict(mod, test))
  absolute <- abs_err(test$blot, preds) %>% round(4)
  rootmean <- rms_err(test$blot, preds) %>% round(4)
  return(list(abs_err = absolute, rms_err = rootmean))
}

random_dose_errs <- compute_errs(m4)
fixed_dose_errs <- compute_errs(m1.full)

tab <- t(data.frame(x=as.numeric(random_dose_errs),y=as.numeric(fixed_dose_errs)))
colnames(tab) <- c("MAE","RMSE")
rownames(tab) <- c("Random Dose + Transformations", "Fixed Dose")
kable(tab)

m5 <- lmer(log(blot) ~ proto * log(body) + (I(1/(dose1+1/2)) + dose2 + 1|lab), data=dat_train, REML=FALSE)
nova5 <- anova(m4, m5)


```

We see that the variability due to sources other than the random effects is 0.0797 (decreased from 0.193). Also, the diagnostic plots improved, although it is notable that the residual variance decreases for larger fitted values. The fixed dose model is also much worse at predicting than the random dose model with transformations. We still see problems with this model, including the non-randomness in the body weight residuals. To counteract this, we added interactions between log body weight and protocol, and an anova test ($p$ = `r round(nova5[["Pr(>Chisq)"]][length(nova[["Pr(>Chisq)"]])], 4)`, $\chi^2$=`r round(nova[["Chisq"]][length(nova[["Chisq"]])], 4)`) confirmed that the interactions improve the model.

##m5 stuff

### Lab Variation

#INCOMPLETE 
The plot below samples random dose1 effects (slopes) and lab effects (intercepts) from the mixed model, holding all other predictors constant, and plots the results. The variance in the dose1 effects by lab is noticeably high, so we say that the bioassay does depend on lab and thus this study fails miserably. 
```{r, echo=FALSE}
nsim <- 1000

reverse_trans <- function(dosetrans){
  return(1/dosetrans - 1/2)
}
# randslopes <- rnorm(nsim, L3[[2]][2,1], sqrt(L3[[2]][2,2])) #%>% reverse_trans()
# randinters <- rnorm(nsim, L3[[2]][1,1], sqrt(L3[[2]][1,2]))
# randlines <- data.frame(a = randinters, b=randslopes, x=seq(-10, 10, length.out=nsim), y=seq(-10, 10, length.out = nsim))
#ggplot(data=randlines, aes(x, y)) + geom_blank() + geom_abline(data=randlines, aes(intercept=a,slope=b), color="gray") + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank(), axis.line = element_line(colour = "black"))

simdose1curves <- function(mod, nsim = 1000){
  
  #simulated data
  L <- lmer_list(mod)
  MEANS <- c(L[["random"]]["(Intercept)", "mean"], L[["random"]][grep("dose1", rownames(L[["random"]])), "mean"])
  
  COR_full <- attr(VarCorr(mod)[[1]], "correlation")

  dose1ind <- grep("dose1", rownames(COR_full))
  interind <- grep("Inter", rownames(COR_full))
  
  COR <- COR_full[c(interind,dose1ind), c(interind,dose1ind)]
  sdxy <- prod(attr(VarCorr(mod)[[1]], "stddev")[c(interind,dose1ind)])
  #cov(x,y) = cor(x,y)*sd(x)sd(y)
  COV <- COR * sdxy
  sims <- rmvnorm(nsim, mean = MEANS, sigma = COV)
  dose1sims <- sims[,2]
  labsims <- sims[,1]
  
  #constant data
  protoeffect <- 0 #just assume baseline protocol
  dose2 <- 0 
  dose2effect <- eval(parse(text = grep("dose2", rownames(L$random), value=T))) #in case dose2 needs to be transformed
  fixedintereffect <- L$fixed["(Intercept)","mean"]
  protoeffect <- 0 
  body <- log(mean(dat_train$body))
  bodyeffect <- log(body)*(L$fixed[grep("body", rownames(L$fixed)), "mean"])
  
  dose1 <- seq(0,10, 0.1)
  dose1trans <- eval(parse(text = grep("dose1", rownames(L$random), value=T)))
  dose1effect <- (apply(t(dose1trans), 2, '*', dose1sims))
  
  response <- exp(fixedintereffect + protoeffect + bodyeffect + dose2effect + labsims + dose1effect)

  plot(c(0,0), xlim = c(min(dose1), max(dose1)), ylim = c(quantile(response, 0.01), quantile(response, 0.99)))
  for(r in 1:nrow(response)){ #for each simulation...
    lines(dose1, response[r,])
  }
}
simdose1curves(m5)
```

<!--
account for correlation using multivariate normal.
use 2 dimensional mvnorm for slope and intercept.
just plug in average values for everything else.
-->

### Conclusion

Because the dose 1 effect has such a high variance relative to the mean, our model shows that the bioassay being studied does not measure a consistent response to dosage across labs.

### Contributions

Nathaniel Brown made the visualizations for this report. He also organized the relevant files in a Github repository for the group to access and edit. Annie Tang compiled the group work done on EDA into a .rmd and wrote the accompanying explanations for the EDA and approaches to analysis. William Yang helped pair on EDA analysis and identify approaches to handle the data. Approaches to analysis were a joint effort by all members of the group. Nathanial implemented analysis for the univariate normal and multivariate normal approaches. Implementation and analysis of the mixed effects model was a joint effort by all members of the group. 

